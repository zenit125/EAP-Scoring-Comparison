---
title: "EAP Scoring Comparison in PROMIS^®^"
author: "Robert Chapman"
date: "10/29/2021"
output: 
  bookdown::word_document2:
    reference_docx: ["https://github.com/zenit125/RMarkdown_Manuscript_Formatting/blob/main/apa_template.docx"]
bibliography: ["~/EAP_Scoring_Comparison/EAP_in_PROMIS_Bibliography.bib"]
csl: ["https://raw.githubusercontent.com/zenit125/RMarkdown_Manuscript_Formatting/main/apa.csl"]
link-citations: true
---


<!--
Northwestern University

Author Note
Robert Chapman, Department of Medical Social Sciences, Feinberg School of Medicine, Northwestern University. 

Correspondence concerning this article should be addressed to Robert Chapman, Department of Medical Social Sciences, Feinberg School of Medicine, Northwestern University, 625 North Michigan Avenue, Chicago, Illinois, 60601. 
Contact: Robert.Chapman@northwestern.edu -->

<!-- new page below for title/coverpage, title is automatically generated by R Markdown -->
\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

```


<!-- 

After knitting:
-Tables title APA, return, italicize & unbold
-table borders for APA style
-Center cover page title
-centering tables & figures after finishing

--->

# Abstract {.unnumbered}

**Background:**  
PROMIS measures offer a plethora of administration options, including multiple short forms per profile domain and multiple ways to score and interpret. However, these same strengths can also hinder the implementation of PROMIS measures, as multiple options can cause confusion around the appropriate or “best” way to select and score PROMIS measures in the contexts of specific populations or diseases, or when data is missing. This study aimed to confirm the equivalence of PROMIS profile domain short forms across multiple research contexts and evaluate scoring methods in the presence of missing data.

**Methods:**  
Analyses were conducted stepwise in three “studies”. Study 1 utilized simulated data (n=1000) derived from item probabilities in each PROMIS profile domain to set hypotheses from observations of concordance between short-forms in each domain, across pattern-response and look-up table scoring methods and in the presence of 1, 2 or 3 randomly induced missing items in each respondent. Study 2 adds datasets containing general population participants (n=2000) and datasets containing clinical patients (e.g., Oncology, n=8474) and replicates the analyses conducted with simulated data to evaluate the hypotheses set in Study 1. Study 3 utilizes the same datasets as Study 2, but evaluates a common method of reducing the impact of missing data at the group-level, mean replacement of missing data. All analyses were conducted with R Statistical Software, version 3.4.1.  

**Results:**   
Study 1: Short-forms scores in each domain showed excellent agreement (ICC 0.95-0.99), and minimal error (RMSE 2.22-4.41) across pattern-response and look-up table scoring methods, when no missing data was present. The lowest error was observed in the longest short form in domain and pattern-response scoring consistently showed lower error terms than look-up table scoring. Once 1, 2 or 3 randomly missing items were induced in simulated data, RMSE rose exponentially with look-up table scoring and linearly in pattern-response scoring. Short forms with more items showed greater stability and less error across scoring methods.
  
Study 2: In general population and clinical datasets, we confirmed our observed hypotheses derived from simulated data: excellent agreement between short forms (ICC 0.93-0.99, 0.90-0.97), and similar error (RMSE 0.41-4.41, 0.59-5.51) when no missing data was present. Inducing missing data replicated the trend of exponential rise in error with look-up table scoring and linear rise in RMSE with pattern response scoring.
  
Study 3: When mean replacement method are applied to the induced missing data in general population and clinical datasets, the error was greatly reduced with the look-up table scoring methods (RMSE 0.49-5.16, 0.88-6.65) but had little benefit for pattern-response scoring (RMSE 1.5-4.78, 1.5-6.77). Short forms scored via look-up table methods and with mean replacement of missing data performed almost as well as short forms scored with pattern-response methods, independent of mean replacement methods. 

# Translational Abstract {.unnumbered}

  

# Introduction {.unnumbered}

The Patient Reported Outcome Measurement Information System (PROMIS®) has been around for 15 years. The PROMIS measurement system was developed as a disease and population cross-cutting measurement system of health-related quality of life developed and scored with item response theory (IRT) . The scientific, clinical and regulatory community has supported the further validation of PROMIS in many different conditions, populations and treatment contexts. There are seven profile domains of Health-Related Quality of Life (HRQL) in PROMIS; emotional distress (depression and anxiety), physical function, fatigue, pain interference, sleep disturbance and satisfaction with participation in social roles. Each domain can be administered as a computer adaptive test (CAT) or as short forms and scored with methods of IRT scoring.  

Explanation of administration and scoring options for PROMIS measures is accessible in layman terms on HealthMeasures.net and in detail in scientific literature. This manuscript seeks to supplement the existing work by providing quantitatively-focused guidance on the comparability of administration and scoring options in PROMS measures. Additional evaluation stratifies results across level of HRQL and statistical methods for evaluating group and individual differences.  

The data and statistical code produced for this manuscript are publicly available and are intended to be used. They are made accessible to the PROMIS community to enable further discussion and research about sample size, power, analytic methods, and the selection of administration and scoring options with PROMIS measures.  

```{r TableOfAbbreviations}
# This R Markdown chunk creates a Kable table of abbreviations used in the tutorial
knitr::kable(data.frame(matrix(ncol=2,byrow=TRUE,dimnames = list(c(),c("Abbreviation","Term")),
 data=c(
    "CAT","Computer Adaptive Test",
    "EAP","Expected A Posteriori",
    "IRT","Item Response Theory",
    "PROMIS","Patient Reported Outcomes Measurement Information System",
    "SD", "Standard Deviation"))),
 caption="Table of Abbreviations")
```

# Methods {.unnumbered}

## Data Generation {.unnumbered}
This work leverages simulation data to answer analytic questions. The reader should be aware that simulated data is not the same as real data obtained from participants or patients. The simulated data used here is built from an estimate, or model, of the data that we would expect from real participants. As a part of their development, PROMIS measures were administered to a large sample of the general population. These data were used to derive IRT calibration statistics using the Graded Response Model. These calibration statistics are most commonly used to score PROMIS measures on their general population centered metric, but they can also be used to estimate how an individual would respond to a PROMIS question or item. The simulated data used here is modelled on real participant responses collected during the development of the PROMIS measures.  

The use of simulation data is appropriate here for three reasons: the scope of the analytic research question in this manuscript, the reliance of the PROMIS scoring methods on it’s model, represented by the IRT calibrations, and the nature of external validity of measurement. Firstly, this work seeks look at sets of large samples of respondents across seven PROMIS profile domains, each of which have their own bank of 50-100 items. Conducting analysis across each of these dimensions requires data from millions of simulated participants. Secondly, when response data from either real or simulated participants is scored using PROMIS IRT scoring methods, the resulting score is anchored on the model estimated from the original calibration sample. Finally, a worthwhile question for the reader to consider is “does my clinical or research sample appropriately represented by the PROMIS model”. The question is really one of the external validity of measurement, about the generalizability across different contexts, such as groups of people, time and cultures. Research literature and HealthMeasure.net documents a growing body of research that PROMIS measures are valid in broad array of contexts, but there can also be additional validation. This work provides additional tools for researchers to conduct additional validation in their specific research contexts, and we hope to see and hear about future work.  

* e.g., population based questions, give the example of sample sd < pop SD  
* description of centers as examples of different pt/individual variance in HRQL  
  
RMSE & mean diff- looks at both variability of error & direction of bias
Simulated data was created from PROMIS item parameters. PROMIS T-scores are calibrated on a United States general population sample, with a T-score of 50 relating to the mean of the calibration sample and 10 T-score points indicating a standard deviation. Analyses focused on samples centered on general population (mean T score of 50) and samples with better and worse health-related quality of life (mean T scores of 40 and 60). From each of the three sample centers, we generated six additional samples that represent a mean change scores of ± 2, 5 and 8 T score points. In total, we analyzed 21 sets of data, one baseline and six change score datasets across three centers. All datasets included 1000 simulated participants based on a normal distribution of T scores with standard deviation of 10 T-score units, in whole T score units ranging from 10-90. These T scores represent the ‘true scores’ of simulated participants or patients, which can be used to evaluate measurement error across administration options (e.g., CAT or Short-Form) and scoring methods.
The ‘true’ T score distribution was used to generate weighted item responses all items across PROMIS profile domains (i.e., Depression, Anxiety, Sleep, Fatigue, Physical Function, Social, Pain) for simulated respondents. Using these full sets of item responses, we generated T scores from a CAT item selection and from short forms using two forms of Expected A Priori scoring, one using pattern response (a.k.a. PR) and the other using sum score to IRT score look-up table (a.k.a. LU). In the simulated data, all Profile domain short forms available on HealthMeasures.net and PROMIS Standard CAT item selection (i.e., MWPI) and stopping rules were used. 

## Evaluation of Score Error  
Observational group-level scoring error was evaluated by comparing means, standard deviations between short form or CAT administration T scores and their ‘true’ population mean score at each center (50, 60, 40 mean T scores) and population standard deviation (10 T score standard deviation). Longitudinal group-level change was evaluated by evaluating effect sizes and change in means between of each center and one of six simulated change levels of ± 2, 5 and 8 T score points.  Individual-level observational scoring error was evaluated by Root Mean Square Error (RMSE) between ‘true’ (i.e., model) T scores and short form or CAT administration T scores (i.e., observed) scores, both as an overall RMSE score and RMSE calculated at individual T score levels in the sample. Longitudinal individual-level scoring error was evaluated as the proportion of respondents identified as changed using an individual-change threshold of 3 T score threshold, using individual change scores between each center and one of six simulated change levels of ± 2, 5 and 8 T score points.  
  
##################
***insert Cohen's & RMSE equation
##################

## Group-Level Scoring Error  
### Observational  
In all domains, group-level means generated for a T score center of 50 showed generally good performance across scoring methods, including short form length, CAT and scoring method (look-up or pattern-response scoring). There was also minimal error in centers that reflect worse health (e.g., 40 center in functional domains, 60 in symptom domains). In centers that reflected better health (e.g., 60 center in functional domains, 40 in symptom domains) there was a consistent underestimation of scores, with mean being biased towards 50 and smaller standard deviations. Across centers and domains, there wasn’t a consistent of meaningful differences in short form scores from pattern-response or look-up table scoring methods and CAT scores had error comparable to the longest short form available in a domain or lower than all short forms.

## Longitudinal 
In all domains, the 50 center showed a small amount of error (mean difference and Cohen’s d) in estimating group-level +5 to -5 T score point change. Across domains, a change from the center of 50 of 8 T score points in the direction of better health showed larger error than a change of 8 T score points in the direction of worse health. Shorter short forms (e.g., 4-item) showed greater mean and effect size scoring error in assessing change than longer short forms, and had even greater error when assessing an improvement of health from the T score center of 50. Across all the centers that reflect worse health, there was much less error, with some bias in high levels of change (8 T score points). The centers that reflect better health show much more bias with change scores reflecting better health showing more bias in mean score and effect size. Across centers, domains and levels of change, the scoring method (look-up table versus pattern-response scoring) did not show a meaningful difference in error and CAT scores were comparable to the longest available short-form or demonstrated the lowest error. 

## Individual-Level Scoring Error  
### Observational – table 3  
Root Mean Square Error (fig x.) was used to evaluate error between individual respondent ‘true’ T score and their T score estimated from a short form or CAT administration. Overall, longer short forms showed lower RMSE’s, and distribution of RMSE’s across T score levels as shown by the standard deviation of RMSE’s and ranger of RMSE’s. The lowest RMSE values and smallest distributions of RMSE’s were found in centers that reflected worse health, closely followed by 50 center. Centers that reflected better health had substantially larger RMSE values and distributions across all scoring methods and short form levels. In all cases, Look-up table scoring and pattern response scoring did not make a meaningful difference in RMSE. CAT administration generally showed better precision (lower overall and smaller distribution of RMSE’s across T scores) with a fewer number of items administered than a short form with an equivalent number of items. 
  
### Longitudinal – table 4
Overall, longer short forms were more able to detect a 3 T score point change. Centers that reflected worse health or a T score center of 50 generally showed better ability to detect change than the centers that reflected better health. There wasn’t a meaningful difference between scoring method (look-up table scoring or pattern-response scoring). 


## Conclusions {.unnumbered}  
  
* There wasn’t an instance where CAT meaningfully underperformed other methods
*	There wasn’t a big difference between LU/PR scoring across all dimensions- but some of that might be due to complete data generated from response probabilities. Response probabilities not generated from the model might be problematic (replication with real data is necessary). Also, there might be an interaction with missing data, and score recovery & scoring method.
* Across domain, method, etc. bias was in the direction of the prior- 50. This might reflect an important limitation of the single method we use. 
* When looking at detecting change, paying attention to short form length and precision are very important, but it’s just as important to pay attention the region where you have reliability. 


\begin{equation}
Probability =
{ \frac{1} {1+e^{-1*discrimination(theta-threshold)}} } 
(\#eq:Probability)
\end{equation}

# List of Appendices and Attachments {.unnumbered}

### ThetaSEeap.R  {.unnumbered}
The "ThetaSEeap.R" script is an R script for calculating "pattern response" EAP scores, and was originally written by @ThetaSEeap.  

### RSSS.R  {.unnumbered}
The "RSSS.R" script is an R script for calculating EAP to raw sum score "Look Up" tables, and was originally written by @RSSS.   


\newpage

# References {.unnumbered}

<div id="refs"></div>


\newpage

# Appendices {.unnumbered}

## ThetaSEeap.R {.unnumbered}

```{r ThetaSEeap, code = readLines("ThetaSEeap.R"), echo=TRUE}

```

\newpage

## RSSS.R {.unnumbered}

```{r RSSS, code = readLines("RSSS.R"), echo=TRUE}

```

